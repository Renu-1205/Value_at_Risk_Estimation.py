# -*- coding: utf-8 -*-
"""Value_at_Risk_Estimation.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PCOkRz7mh1A4Ae5ur_nlTSjBZOddOjQW

##**Value at Risk (VaR) Analysis using Statistical Methods**

Introduction

Value at Risk (VaR) is a key measure used in financial risk management to estimate potential losses in an investment portfolio over a given time period at a specified confidence level. This analysis utilizes various approaches, including historical simulation, bootstrapping, and Monte Carlo simulations using a t-distribution.
"""

import datetime as dt
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.mlab as mlab
import yfinance as y
import scipy.stats as stats
from scipy.stats import (
    norm,
    kurtosis,
    skew,
    jarque_bera,
    normaltest)

# Define stock tickers and date range
tickers = ["AAPL", "MSFT", "GOOGL", "AMZN"]
start_date = "2015-04-01"
end_date = "2025-04-01"

# Download data
data = yf.download(tickers, start=start_date, end=end_date)

# check columns
print(data.head())

# column names to understand the structure
print(data.columns)

# Extract only the 'Close' prices
closing_prices = data["Close"]

# Display first few rows
print(closing_prices.head())

# Get the dimensions of the dataset
num_rows, num_cols = closing_prices.shape

# Display dimensions
print(f"Dataset Dimensions: {num_rows} rows, {num_cols} columns")

# Check for missing values in the dataset
missing_values = data.isnull().sum()
print("Missing values per column:\n", missing_values[missing_values > 0])

# Check for duplicate rows
duplicate_rows = data.duplicated().sum()
print("\nNumber of duplicate rows:", duplicate_rows)

# Equal allocation (25% each stock)
weights = np.array([0.25, 0.25, 0.25, 0.25])
print("Investment Weights:", weights)

total_investment = 100000

# Compute log returns
log_returns = np.log(1 + closing_prices.pct_change()).dropna()

# Display first few rows
print(log_returns.head())

# Set style
plt.style.use('seaborn-v0_8-whitegrid')
fig, ax = plt.subplots(figsize=(14, 5))

# Plot DAILY log returns
for ticker in log_returns.columns:
    ax.plot(log_returns[ticker],
            linewidth=0.8,
            alpha=0.7,
            label=ticker)

# Customize
ax.set_title('Log Returns', fontsize=13, pad=15)
ax.set_xlabel('Date', fontsize=11)
ax.set_ylabel('Log Return', fontsize=11)
ax.axhline(0, color='black', linestyle=':', linewidth=0.5)  # Zero line

# Highlight extreme moves
ax.fill_between(log_returns.index,
                log_returns.min(axis=1),
                0,
                where=(log_returns.min(axis=1) < 0),
                color='red', alpha=0.1)
ax.fill_between(log_returns.index,
                0,
                log_returns.max(axis=1),
                where=(log_returns.max(axis=1) > 0),
                color='green', alpha=0.1)

# Formatting
ax.legend(fontsize=9, bbox_to_anchor=(1.02, 1))
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

from scipy.stats import jarque_bera
import pandas as pd

def test_normality(series, alpha=0.05):
    """Performs the Jarque-Bera normality test on a pandas Series."""
    jb_stat, jb_p = jarque_bera(series)

    is_normal = jb_p > alpha  # If p > 0.05, data is likely normal

    return pd.Series({
        'JB_p': jb_p,
        'Is_Normal': is_normal
    })

# Apply JB test to all assets' LOG RETURNS
normality_results = log_returns.apply(test_normality)
print("Normality Test Results (Log Returns):\n", normality_results.T)

# Set up the figure
plt.figure(figsize=(14, 10))
plt.suptitle('Log Returns Distribution vs Normal', y=1.02, fontsize=14)

# Plot each stock's distribution
for i, ticker in enumerate(log_returns.columns, 1):
    plt.subplot(2, 2, i)

    # 1. Plot actual returns (histogram + KDE)
    sns.histplot(log_returns[ticker], bins=50, kde=True,
                 stat='density', color='royalblue', alpha=0.4,
                 label='Actual Returns')

    # 2. Plot normal distribution with same mean/std
    mu, sigma = log_returns[ticker].mean(), log_returns[ticker].std()
    x = np.linspace(mu - 4*sigma, mu + 4*sigma, 100)
    plt.plot(x, norm.pdf(x, mu, sigma), 'r-', linewidth=2,
             label=f'Normal ($μ$={mu:.4f}, $σ$={sigma:.4f})')

    # Formatting
    plt.title(ticker, fontweight='bold')
    plt.xlabel('Daily Log Return')
    plt.legend(fontsize=9, framealpha=0.9)

plt.tight_layout()
plt.show()

confidence_level = 0.99  # 99% VaR
# 1. Calculate weighted portfolio log returns
portfolio_log_returns = (log_returns * weights).sum(axis=1)

# 2. Compute Historical VaR (1st percentile)
historical_var_percentile = np.percentile(portfolio_log_returns, 100*(1-confidence_level))
historical_var_dollar = abs(historical_var_percentile) * total_investment

# 3. Print results
print(f"Historical VaR ({confidence_level:.0%} confidence):")
print(f"- Worst daily log return: {historical_var_percentile:.4f}")
print(f"- Dollar amount: ${historical_var_dollar:,.2f}")

"""Historical VaR estimates potential losses based on past return distributions without assuming any parametric model."""

import numpy as np
import matplotlib.pyplot as plt

def bootstrap_var(returns, weights, initial_investment=100000, n_simulations=1000, confidence=0.99):
    """Calculate bootstrapped VaR for a portfolio without distributional assumptions"""
    portfolio_returns = returns @ weights
    var_percentile = 100 * (1 - confidence)

    # Bootstrap resampling
    bs_replicates = np.array([
        np.percentile(np.random.choice(portfolio_returns, size=len(portfolio_returns), replace=True),
                      var_percentile)
        for _ in range(n_simulations)
    ])

    return abs(bs_replicates) * initial_investment

# Run bootstrap
bs_results = bootstrap_var(log_returns, weights)

# Calculate statistics
mean_var = np.mean(bs_results)
ci_lower_99, ci_upper_99 = np.percentile(bs_results, [0.5, 99.5])

# Print results
print(f"Bootstrap 99% VaR Results:")
print(f"Mean Estimate: ${mean_var:,.2f}")
print(f"99% Confidence Interval: [${ci_lower_99:,.2f}, ${ci_upper_99:,.2f}]")

plt.figure(figsize=(10, 5))
plt.hist(bs_results, bins=30, color='steelblue', alpha=0.7)
plt.axvline(ci_lower_99, color='green', linestyle='--', label='99% Worst Case (VaR)')  # Only left-side bound
plt.title("Bootstrap VaR Distribution")
plt.xlabel("Potential Loss ($)")
plt.ylabel("Frequency")
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

"""Bootstrapping helps in estimating VaR by resampling past data multiple times to improve robustness."""

# Assuming log_returns is a DataFrame with 4 columns (one for each stock)
skewness = log_returns.apply(stats.skew, axis=0)  # Compute skewness for each stock
kurtosis = log_returns.apply(stats.kurtosis, axis=0, fisher=True)  # Compute excess kurtosis

# Create a DataFrame to display results
stats_df = pd.DataFrame({'Skewness': skewness, 'Kurtosis': kurtosis})
print(stats_df)

# Interpretation
for stock in log_returns.columns:
    print(f"\nStock: {stock}")
    print(f"Skewness: {stats_df.loc[stock, 'Skewness']:.4f}")
    print(f"Kurtosis: {stats_df.loc[stock, 'Kurtosis']:.4f} (Excess Kurtosis)")

    if abs(stats_df.loc[stock, 'Skewness']) < 0.5 and abs(stats_df.loc[stock, 'Kurtosis']) < 0.5:
        print(" → Data is approximately normal.")
    elif stats_df.loc[stock, 'Kurtosis'] > 3:
        print(" → Heavy-tailed data detected. Consider Student’s t-distribution.")
    elif stats_df.loc[stock, 'Skewness'] > 0:
        print(" → Right-skewed data detected. Consider Log-Normal distribution.")
    elif stats_df.loc[stock, 'Skewness'] < 0:
        print(" → Left-skewed data detected. Consider Skew-Normal distribution.")

import numpy as np
import scipy.stats as stats

def monte_carlo_t_var(returns, weights, initial_investment=100000, n_simulations=1000, confidence=0.99):
    """
    Monte Carlo VaR using Student’s t-distribution
    Args:
        returns: DataFrame of log returns (n_days x n_assets)
        weights: Portfolio allocation weights
        initial_investment: Portfolio value in dollars
        n_simulations: Number of Monte Carlo simulations
        confidence: VaR confidence level
    Returns:
        Monte Carlo VaR estimate in dollars
    """
    # Step 1: Estimate t-distribution parameters for each stock
    t_params = [stats.t.fit(returns[col]) for col in returns.columns]

    # Step 2: Generate correlated t-distributed returns
    corr_matrix = returns.corr().values  # Correlation matrix
    chol = np.linalg.cholesky(corr_matrix)  # Cholesky decomposition

    # Step 3: Monte Carlo simulations
    var_list = []
    for _ in range(n_simulations):
        # Simulate independent t-distributed returns for each stock
        simulated_returns = np.array([
            stats.t(df, loc, scale).rvs(252)  # Simulate 252 trading days
            for df, loc, scale in t_params
        ])

        # Apply correlation using Cholesky decomposition
        correlated_returns = chol @ simulated_returns

        # Compute portfolio returns
        portfolio_returns = weights @ correlated_returns

        # Compute VaR at given confidence level
        var = -np.percentile(portfolio_returns, 100 * (1 - confidence))
        var_list.append(var * initial_investment)

    return np.mean(var_list)

# Run Monte Carlo using Student’s t-distribution
mc_t_var = monte_carlo_t_var(log_returns, weights)
print(f"Monte Carlo 99% VaR (t-distribution): ${mc_t_var:,.2f}")

"""

This analysis demonstrates different methods of VaR estimation and their respective implications:

Historical VaR provides a straightforward estimation but relies on past data.

Bootstrap VaR offers a more refined estimation by resampling from past returns.

Monte Carlo VaR using t-distribution accounts for fat tails and correlations, making it more realistic for financial risk assessment.

Using these methods, we obtain a robust understanding of portfolio risk, which is critical for effective financial decision-making and risk management.

"""